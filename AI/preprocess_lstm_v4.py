import pandas as pd
import numpy as np
import glob
import os
import pickle
from sklearn.preprocessing import MinMaxScaler
from tqdm import tqdm

# ==========================================
# âš™ï¸ ì„¤ì •ê°’
# ==========================================
DATA_DIR = 'data/SisFall_dataset'  # ì„œë²„ ë‚´ ì›ë³¸ í´ë”ëª… í™•ì¸ í•„ìš”!
SAVE_PATH = 'data/lstm_data_v4.pkl' # V4 ì €ì¥
SCALER_PATH = 'data/scaler.pkl'

ORIGIN_HZ = 200
TARGET_HZ = 50
DOWN_STEP = 4
WINDOW_SIZE = 150 

def process_smart(filepath, label):
    try:
        df = pd.read_csv(filepath, header=None, engine='python')
        if df.shape[1] < 9:
            df = pd.read_csv(filepath, header=None, sep=r'[,;]', engine='python')

        df = df.iloc[:, [0, 1, 2, 3, 4, 5]] # 6ì¶•
        df.columns = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']
        df = df.iloc[::DOWN_STEP, :].reset_index(drop=True)

        segments = []
        labels = []

        # CASE 1: ë‚™ìƒ(1) - Peak Centric ìœ ì§€
        if label == 1:
            svm = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)
            peak_idx = svm.idxmax()
            start_idx = peak_idx - (WINDOW_SIZE // 2)
            end_idx = start_idx + WINDOW_SIZE
            
            if start_idx >= 0 and end_idx <= len(df):
                segments.append(df.iloc[start_idx:end_idx].values)
                labels.append(1)
                # ì¦ê°• 1ê°œ ì¶”ê°€ (ì•ìœ¼ë¡œ 0.2ì´ˆ ë‹¹ê²¨ì„œ)
                # segments.append(df.iloc[start_idx-10:end_idx-10].values)
                # labels.append(1)

        # CASE 2: ì¼ìƒ(0) - ğŸ”¥ ìˆ˜ì •ëœ ë¶€ë¶„
        else:
            # ê¸°ì¡´ 150 -> 50ìœ¼ë¡œ ë³€ê²½ (3ë°° ë” ì´˜ì´˜í•˜ê²Œ!)
            # ë‚™ìƒ ë°ì´í„°ë³´ë‹¤ ì¼ìƒ ë°ì´í„°ê°€ ë” ë§ì•„ì§€ê²Œ ìœ ë„í•¨
            stride = 50 
            for i in range(0, len(df) - WINDOW_SIZE, stride):
                window = df.iloc[i : i + WINDOW_SIZE].values
                segments.append(window)
                labels.append(0)

        return segments, labels

    except Exception as e:
        return [], []

if __name__ == "__main__":
    # ê²½ë¡œ ì£¼ì˜: ì„œë²„ì— ì••ì¶• í‘¼ í´ë”ëª…ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš” (ì˜ˆ: data/SisFall_dataset)
    file_list = glob.glob(os.path.join(DATA_DIR, "**", "*.txt"), recursive=True)
    file_list = [f for f in file_list if "readme" not in f.lower()]
    
    print(f"ğŸ”„ V4 ë°ì´í„° ì¦ê°• ì „ì²˜ë¦¬ ì‹œì‘... (ì´ {len(file_list)}ê°œ íŒŒì¼)")

    all_X = []
    all_y = []

    for filepath in tqdm(file_list):
        filename = os.path.basename(filepath)
        if filename.upper().startswith('D'): label = 0
        elif filename.upper().startswith('F'): label = 1
        else: continue

        segs, lbls = process_smart(filepath, label)
        if len(segs) > 0:
            all_X.extend(segs)
            all_y.extend(lbls)

    X = np.array(all_X)
    y = np.array(all_y)

    print(f"\nğŸ§© ë°ì´í„° ëª¨ì–‘: {X.shape}")
    print(f"ğŸ“Š ë‚™ìƒ(1): {sum(y)}ê°œ, ì¼ìƒ(0): {len(y)-sum(y)}ê°œ") 
    # ëª©í‘œ: ì¼ìƒ(0) ê°œìˆ˜ê°€ ë‚™ìƒ(1)ë³´ë‹¤ ë¹„ìŠ·í•˜ê±°ë‚˜ ë§ì•„ì•¼ í•¨!

    # ìŠ¤ì¼€ì¼ë§
    N, T, F = X.shape
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X.reshape(N * T, F)).reshape(N, T, F)

    with open(SAVE_PATH, 'wb') as f:
        pickle.dump((X_scaled, y), f)
    
    import joblib
    joblib.dump(scaler, SCALER_PATH)
    print("âœ… V4 ì™„ë£Œ!")